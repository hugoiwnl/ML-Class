{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4be961-6815-40bc-87d2-04eb17acbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a5c0f4-ea2f-4d06-ab00-c1c8f2f14cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits['data'], digits['target'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20af9df-a34c-4152-ab3d-00e70fba1113",
   "metadata": {},
   "source": [
    "# Task 1 - Kernelized SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3afd5a-6090-418c-b1bf-61ea64b3b835",
   "metadata": {},
   "source": [
    "## Subtask 1 - SVMs With Different Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1641088-3e40-4080-be2c-9e0d01a8ec7c",
   "metadata": {},
   "source": [
    "Here I define a function which I will use for every kernel, where I will compute the model's performance with a specific kernel without any tuning of the hyperparameters. I compute the accuracy score on both trening and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c71dd50-25d7-46f7-93c9-e631e148386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_no_tuning(ker):\n",
    "    # Calculting the test accuracy\n",
    "    svm_model = SVC(kernel=ker)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    y_pred_test = svm_model.predict(X_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"Kernel test accuracy before tuning: {accuracy_test}\")\n",
    "    \n",
    "    # Calculating the training accuracy\n",
    "    y_pred_train = svm_model.predict(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    print(f\"Kernel train accuracy before tuning: {accuracy_train}\")\n",
    "    return accuracy_test, accuracy_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b0535-32fc-4101-a0b9-6d78307ac93e",
   "metadata": {},
   "source": [
    "### Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22652d33-10fc-4c3c-898f-a47c0aac1b2b",
   "metadata": {},
   "source": [
    "In the first part, I calculate the model accuracy for training and test data without tuning the hyperparameter C (regulizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003c7be5-ad1d-49bc-bbdc-6f41244cc921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel test accuracy before tuning: 0.9796296296296296\n",
      "Kernel train accuracy before tuning: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Printing the initial accuracies without tuning the model and saving them for later analysis\n",
    "nt_linear_test, nt_linear_train = accuracy_no_tuning(\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1c0de-11e3-4ca1-994b-31d6e294d3b2",
   "metadata": {},
   "source": [
    "For finding the optimal hyperparameter, I will try different values of C and calculate 10-fold cross validations scores over training data to find it. For simplicity, I choose the c values that are in the 4th line, which may not be the most optimal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd1a4e6-5755-4516-b18e-b15423104490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value for hyperparameter C: 0.001\n",
      "Test Accuracy after tuning the model: 0.9833333333333333\n",
      "Train Accuracy after tuning the model: 0.9904534606205251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Different C values that I used for finding the optimal hyperparameter\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Making a dictionary to store the cross validations scores for each C value\n",
    "results = {}\n",
    "\n",
    "for C in C_values:\n",
    "    # Initialize SVM with a linear kernel and the current C value\n",
    "    svm_linear = SVC(kernel='linear', C=C)\n",
    "    \n",
    "    # Perform cross-validation (here, using 10-fold cross-validation)\n",
    "    cross_val_scores = cross_val_score(svm_linear, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    \n",
    "    # Store the average cross-validation accuracy for the current C value\n",
    "    results[C] = np.mean(cross_val_scores)\n",
    "\n",
    "# Find the C value that gives the highest average cross-validation accuracy\n",
    "best_C = max(results, key=results.get)\n",
    "\n",
    "# Train the final model with the best C value on the entire training set\n",
    "best_svm_linear = SVC(kernel='linear', C=best_C)\n",
    "best_svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_preds = best_svm_linear.predict(X_test)\n",
    "test_accuracy_linear = accuracy_score(y_test, test_preds)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_preds = best_svm_linear.predict(X_train)\n",
    "train_accuracy_linear = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Print results\n",
    "print(\"The optimal value for hyperparameter C:\", best_C)\n",
    "print(\"Test Accuracy after tuning the model:\", test_accuracy_linear)\n",
    "print(\"Train Accuracy after tuning the model:\", train_accuracy_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616376c-0ed9-43db-8420-848604c902d7",
   "metadata": {},
   "source": [
    "With tuned linear kernel, I get the training accuracy of 0.9904534606205251 and testing accuracy of 0.9833333333333333 with the optimal parameter C having a value of 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677443e-f669-43f1-911e-2399871aad75",
   "metadata": {},
   "source": [
    "### Polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9dab7c-d109-416b-8cdb-8383507215be",
   "metadata": {},
   "source": [
    "First I compute the training and test accuracy of the model without tuning the parameters (degree and C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f650a8fb-5c54-41d6-b787-ef6c88825e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel test accuracy before tuning: 0.9888888888888889\n",
      "Kernel train accuracy before tuning: 0.9992044550517104\n"
     ]
    }
   ],
   "source": [
    "nt_poly_test, nt_poly_train = accuracy_no_tuning(\"poly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd59c83-2ff8-4298-8136-3cc303880126",
   "metadata": {},
   "source": [
    "Without tuning the optimal parameters (having the default ones for the kernel), I get a test accuracy of 0.9888888888888889, and a train accuracy of 0.9992044550517104."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac8a74-1962-45e8-a7c5-c1f797551765",
   "metadata": {},
   "source": [
    "Here I Try to find the optimal hyperparameters (degree and C) for the best accuracy of the model using 10-cross validation, and apart from iterating over C values that I already mentioned, I also iterate over degree values from 1 to 8 to find the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4084c55-1479-4ac8-b8c7-1e3efb2176e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = [1,2,3,4,5,6, 7, 8]\n",
    "scores = {}\n",
    "\n",
    "# I iterate over all the c values I defined\n",
    "for c in C_values:\n",
    "    # I also iterate over all the degrees \n",
    "    for d in degree:\n",
    "        # First I define a model with those hyper parameters\n",
    "        svm_model = SVC(kernel='poly', C=c, degree=d) \n",
    "        \n",
    "        # Perform cross-validation (here, using 10-fold cross-validation)\n",
    "        cross_val_scores = cross_val_score(svm_model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        \n",
    "        # Store the average cross-validation accuracy for the current C value\n",
    "        scores[(c, d)] = np.mean(cross_val_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420bc999-336e-4f36-9e80-c6bffc2e27bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value C: 10, and the optimal degree is: 3\n",
      "Test Accuracy after tuning the model: 0.9888888888888889\n",
      "Train Accuracy after tuning the model: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Find the C value and degree that gives the highest average cross-validation accuracy\n",
    "best_parameters = max(scores, key=scores.get)\n",
    "\n",
    "# Train the final model with the best C value on the entire training set\n",
    "best_svm_poly = SVC(kernel='poly', C=best_parameters[0], degree=best_parameters[1])\n",
    "best_svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_preds = best_svm_poly.predict(X_test)\n",
    "test_accuracy_poly = accuracy_score(y_test, test_preds)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_preds = best_svm_poly.predict(X_train)\n",
    "train_accuracy_poly = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Print results\n",
    "print(f\"The optimal value C: {best_parameters[0]}, and the optimal degree is: {best_parameters[1]}\")\n",
    "print(\"Test Accuracy after tuning the model:\", test_accuracy_poly)\n",
    "print(\"Train Accuracy after tuning the model:\", train_accuracy_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874520c-844d-4651-934f-9cfe03147c6a",
   "metadata": {},
   "source": [
    "The optimal C value I get is 10, while the optimal degree is 3. With them, I achieve test accuracy of 0.9888888888888889, and a training accuracy of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23b8ef-7e44-4985-8d00-a17cd0dbed95",
   "metadata": {},
   "source": [
    "### RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204a34d-f633-4424-bd7f-10247732c23a",
   "metadata": {},
   "source": [
    "First I compute the training and test accuracy of the model without tuning the kernel parameters (gamma and C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3004d5d-2c07-4b55-89e3-84c4c2c463bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel test accuracy before tuning: 0.987037037037037\n",
      "Kernel train accuracy before tuning: 0.9968178202068417\n"
     ]
    }
   ],
   "source": [
    "nt_rbf_test, nt_rbf_train = accuracy_no_tuning(\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c925c6-af95-4f35-b723-cfc484dc777a",
   "metadata": {},
   "source": [
    "Without tuning the parameters, I get a test accuracy of 0.987037037037037 and a training accuracy of 0.9968178202068417."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3723de1-773a-4847-ac16-2f1eb1ce4bbd",
   "metadata": {},
   "source": [
    "For tuning the model, I try to find the optimal gamma value by iterating over the values that I defined in the next line, along with the C value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71f1773-9367-4cea-9f26-71c280c6409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of parameter C: 10, and the optimal value of parameter gamma is: 0.001\n",
      "Test Accuracy after tuning the model: 0.9907407407407407\n",
      "Train Accuracy after tuning the model: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Possible optimal gamma parameter values for our kernel which I test out\n",
    "gamma_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Iterate over parameter combinations\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        # Initialize SVM with an RBF kernel and the current parameters\n",
    "        svm_rbf = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "        \n",
    "        # Perform cross-validation (here, using 5-fold cross-validation)\n",
    "        cross_val_scores = cross_val_score(svm_rbf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        \n",
    "        # Store the average cross-validation accuracy for the current parameters\n",
    "        results[(C, gamma)] = np.mean(cross_val_scores)\n",
    "\n",
    "# Find the parameters that give the highest average cross-validation accuracy\n",
    "best_params = max(results, key=results.get)\n",
    "\n",
    "# Train the final model with the best parameters on the entire training set\n",
    "best_svm_rbf = SVC(kernel='rbf', C=best_params[0], gamma=best_params[1])\n",
    "best_svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_preds = best_svm_rbf.predict(X_test)\n",
    "test_accuracy_rbf = accuracy_score(y_test, test_preds)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_preds = best_svm_rbf.predict(X_train)\n",
    "train_accuracy_rbf = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Print results\n",
    "print(f\"The optimal value of parameter C: {best_params[0]}, and the optimal value of parameter gamma is: {best_params[1]}\")\n",
    "print(\"Test Accuracy after tuning the model:\", test_accuracy_rbf)\n",
    "print(\"Train Accuracy after tuning the model:\", train_accuracy_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9033a9-fa9f-479d-b82e-d6804e96e12f",
   "metadata": {},
   "source": [
    "The optimal value of C I computed is 10, while the optimal gamma value is 0.001. With them, I got a test accuracy of 0.9907407407407407, and a training accuracy of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829807bb-e5bc-4f67-9cab-d945e386e498",
   "metadata": {},
   "source": [
    "### Sigmoid kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568547a-9900-4fd4-8d91-a1f3a5d332d4",
   "metadata": {},
   "source": [
    "First I compute the training and test accuracy of the model without tuning the kernel parameters (gamma and C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec5b5e2-2a30-483a-bc39-418f1c2ca661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel test accuracy before tuning: 0.9074074074074074\n",
      "Kernel train accuracy before tuning: 0.9196499602227526\n"
     ]
    }
   ],
   "source": [
    "nt_sigmoid_test, nt_sigmoid_train = accuracy_no_tuning(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5494f1d-d023-4a52-9729-d1f0194a6cf6",
   "metadata": {},
   "source": [
    "Without the tuning I get the test accuracy of 0.9074074074074074 and a training accuracy of 0.9196499602227526."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf1c27-223f-49ee-b48f-0c88d5c1a948",
   "metadata": {},
   "source": [
    "For finidg the optimal C value I again iterate over it's values in the array I defined earlier, and for the gamma parameter I just set it to 'scale' as it was suggested on sklearn's website. With iterating over gamma values from my array, I get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c30443a-a4c0-4629-bb42-b881e43f9aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value for hyperparameter C: 1\n",
      "Test Accuracy after tuning the model: 0.9074074074074074\n",
      "Train Accuracy after tuning the model: 0.9196499602227526\n"
     ]
    }
   ],
   "source": [
    "# Making a dictionary to store the cross validations scores for each C value\n",
    "results = {}\n",
    "\n",
    "for C in C_values:\n",
    "        # Initialize SVM with a linear kernel and the current C value, while the optimal gamma value can be computed with scale (from the sklearn website)\n",
    "        svm_sigmoid = SVC(kernel='sigmoid', C=C, gamma='scale')\n",
    "        \n",
    "        # Perform cross-validation (here, using 10-fold cross-validation)\n",
    "        cross_val_scores = cross_val_score(svm_sigmoid, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        \n",
    "        # Store the average cross-validation accuracy for the current C value\n",
    "        results[C] = np.mean(cross_val_scores)\n",
    "\n",
    "# Find the C value that gives the highest average cross-validation accuracy\n",
    "best_C = max(results, key=results.get)\n",
    "\n",
    "# Train the final model with the best C value on the entire training set\n",
    "best_svm_sigmoid = SVC(kernel='sigmoid', C=best_C, gamma='scale')\n",
    "best_svm_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_preds = best_svm_sigmoid.predict(X_test)\n",
    "test_accuracy_sigmoid = accuracy_score(y_test, test_preds)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_preds = best_svm_sigmoid.predict(X_train)\n",
    "train_accuracy_sigmoid = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Print results\n",
    "print(\"The optimal value for hyperparameter C:\", best_C )\n",
    "print(\"Test Accuracy after tuning the model:\", test_accuracy_sigmoid)\n",
    "print(\"Train Accuracy after tuning the model:\", train_accuracy_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018695c-738a-425b-8bb2-ff03062a3c20",
   "metadata": {},
   "source": [
    "With the optimal value of 1 for c, I get a test accuracy of 0.9074074074074074 and a training accuracy of 0.9196499602227526."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae571e2-c24d-480d-9125-6675b61d9fb2",
   "metadata": {},
   "source": [
    "#### After tuning all the models, I will display the achieved results with each kernel, printing the acuracy before and after tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d4e1c9-0832-4649-9c5d-2ff577f645dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Kernel | Test Accuracy | Train Accuracy | Tuned Test Accuracy  | Tuned Train Accuracy |\n",
      "|--------|---------------|----------------|----------------------|----------------------|\n",
      "| Linear | 0.97963       | 1.00000        | 0.98333              | 0.99045              |\n",
      "| Poly   | 0.98889       | 0.99920        | 0.98889              | 1.00000              |\n",
      "| RBF    | 0.98704       | 0.99682        | 0.99074              | 1.00000              |\n",
      "| Sig    | 0.9074        | 0.91965        | 0.90741              | 0.91965              |\n"
     ]
    }
   ],
   "source": [
    "# Displaying the results\n",
    "print(\"| Kernel | Test Accuracy | Train Accuracy | Tuned Test Accuracy  | Tuned Train Accuracy |\")\n",
    "print(\"|--------|---------------|----------------|----------------------|----------------------|\")\n",
    "print(f\"| Linear | {nt_linear_test:.5f}       | {nt_linear_train:.5f}        | {test_accuracy_linear:.5f}              | {train_accuracy_linear:.5f}              |\")\n",
    "print(f\"| Poly   | {nt_poly_test:.5f}       | {nt_poly_train:.5f}        | {test_accuracy_poly:.5f}              | {train_accuracy_poly:.5f}              |\")\n",
    "print(f\"| RBF    | {nt_rbf_test:.5f}       | {nt_rbf_train:.5f}        | {test_accuracy_rbf:.5f}              | {train_accuracy_rbf:.5f}              |\")\n",
    "print(f\"| Sig    | {nt_sigmoid_test:.4f}        | {nt_sigmoid_train:.5f}        | {test_accuracy_sigmoid:.5f}              | {train_accuracy_sigmoid:.5f}              |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beca934-7793-41a9-953f-de5fd3b963b5",
   "metadata": {},
   "source": [
    "#### Also, we are interested in the differences of accuracies of tuned and not tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2142e85-cbfb-4204-939f-e91975d20f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Kernel | Test Difference | Train Difference |\n",
      "|--------|-----------------|------------------|\n",
      "| Linear | 0.00370         | -0.00955         |\n",
      "| Poly   | 0.00000         | 0.00080          |\n",
      "| RBF    | 0.00370         | 0.00318          |\n",
      "| Sig    | 0.00000         | 0.00000          |\n"
     ]
    }
   ],
   "source": [
    "print(\"| Kernel | Test Difference | Train Difference |\")\n",
    "print(\"|--------|-----------------|------------------|\")\n",
    "print(f\"| Linear | {test_accuracy_linear - nt_linear_test:.5f}         | {train_accuracy_linear - nt_linear_train:.5f}         |\")\n",
    "print(f\"| Poly   | {test_accuracy_poly - nt_poly_test:.5f}         | {train_accuracy_poly - nt_poly_train:.5f}          |\")\n",
    "print(f\"| RBF    | {test_accuracy_rbf - nt_rbf_test:.5f}         | {train_accuracy_rbf - nt_rbf_train:.5f}          |\")\n",
    "print(f\"| Sig    | {test_accuracy_sigmoid - nt_sigmoid_test:.5f}         | {train_accuracy_sigmoid - nt_sigmoid_train:.5f}          |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15209da8-817a-47f7-a0c5-bbdfef0f4c9f",
   "metadata": {},
   "source": [
    "We can see that for the test data accuracies we either achieve greater accuracy or remain at the same, but for training accuracy, we lose some on the training data using linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8092cb7f-ad8e-4e5c-be61-4aa69fd3689c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy differences aquired by tuning the kernels')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ker = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "test = [0.00370, 0, 0.00370, 0]\n",
    "train = [-0.00955, 0.0080, 0.00318, 0]\n",
    "\n",
    "plt.scatter(ker, test, color='g', label=\"Test Accuracy\")\n",
    "plt.scatter(ker, train, color='r', label=\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Kernel\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy differences aquired by tuning the kernels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3235a9-3fde-4fad-98c7-186eb4fa8664",
   "metadata": {},
   "source": [
    "## Subtask 2 - Custom kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e9c7c-ee5a-4aa3-8a93-1086f149bfa9",
   "metadata": {},
   "source": [
    "The approach that I had with this task was to try to combine kernels that we implemented with optimal parametes to try to get better accuracy. I tried some of them by brute force (as explained on the lectures), but I failed to achieve greater accuracy than tuned RBF kernel (my best performing model). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b511938-552e-41e7-97bd-e7c22d59c7ab",
   "metadata": {},
   "source": [
    "First I tried to combine RBF and linear kernels with given parameters, and the accuracy I got is 0.98148."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5e7b201-03ee-4cef-b28c-15252f1f0f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98148\n"
     ]
    }
   ],
   "source": [
    "# Defining a custom kernel as a combination of RBF and linear kernels\n",
    "def custom_kernel1(X1, X2, c=10, h=1):\n",
    "    rbf_part = c * np.exp(-np.sum((X1[:, np.newaxis, :] - X2) ** 2, axis=2) / (2 * h**2))\n",
    "    linear_part = np.dot(X1, X2.T)\n",
    "    return rbf_part + linear_part\n",
    "\n",
    "# Create an SVM classifier with the custom kernel\n",
    "svm = SVC(kernel=custom_kernel1)\n",
    "\n",
    "# Train the SVM model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662e1fe-9f91-4c3c-a97a-cfcb50a71411",
   "metadata": {},
   "source": [
    "Then I tried combining polynomial and linear kernel, which just gave me a same result as a non-tuning polynomial kernel with accuracy of 0.98889."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac0837fa-1d3a-4984-83e0-1309b1cd3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98889\n"
     ]
    }
   ],
   "source": [
    "# Defining a custom kernel as a combination of polynomial and linear kernels\n",
    "def custom_kernel2(X1, X2):\n",
    "    poly_part =  polynomial_kernel(X1, X2)\n",
    "    linear_part =   linear_kernel(X1, X2)\n",
    "    return poly_part + linear_part\n",
    "\n",
    "# Create an SVM classifier with the custom kernel\n",
    "svm = SVC(kernel=custom_kernel2)\n",
    "\n",
    "# Train the SVM model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09811624-3db2-437e-a666-bb984883a8db",
   "metadata": {},
   "source": [
    "And for the last kernel I tried combining the RBF and polynomial kernels as my two best performing ones, but the accuracy I got doesn't differ from the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae74f2b0-67b1-4222-bb4c-69bab8ab9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98889\n"
     ]
    }
   ],
   "source": [
    "# Define a custom kernel as a combination of RBF and polynomial kernels\n",
    "def custom_kernel3(X1, X2):\n",
    "    rbf_part =  rbf_kernel(X1, X2)\n",
    "    poly_part =   polynomial_kernel(X1, X2)\n",
    "    return rbf_part + poly_part\n",
    "\n",
    "# Create an SVM classifier with the custom kernel\n",
    "svm = SVC(kernel=custom_kernel3)\n",
    "\n",
    "# Train the SVM model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371a0ec-1921-402e-b3a6-e0a9efc3e8c6",
   "metadata": {},
   "source": [
    "We already proved in the last pen and paper exercise that the sum/multiplication of valid kernels are also valid kernels, so each of the custom kernels I implemented must be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b44e13-cf44-46ca-8fea-81c830cbc892",
   "metadata": {},
   "source": [
    "The idea behind the kernels came from the lectures, where professor discussed that we should either have some kernel based on our domain knowledge about the dataset or try to find a suitable one as a combination of different kernels with brute force."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f53d36-4dd7-4fe2-96e4-da9d46a38741",
   "metadata": {},
   "source": [
    "# Task 2 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98646227-47b4-47c7-aad2-d796c801e2cc",
   "metadata": {},
   "source": [
    "In the first part I normalize the data with MinMaxScaler, and after that I split it into training and test data with shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71bee7be-919a-4b75-8c21-85b69a8df28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into 70% training and 30% test with shuffling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5e220-c1d2-47ad-87a7-974f05dfb509",
   "metadata": {},
   "source": [
    "### Subtask 1 - Forward Greedy Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a089c45-8c64-4988-ae01-28b5c9d7b831",
   "metadata": {},
   "source": [
    "In this subtask I have to implement 'Forward greedy feature selection'. Forward greedy feature selection is a heuristic algorithm employed to iteratively build a subset of relevant features that optimizes a chosen performance metric. The process begins with an empty set and progressively adds the most promising feature at each step, evaluating the impact on the model's performance. The way I evaluate the impact is by calculating a 10-fold cross validation. This iterative selection continues until a predetermined criterion is met. The criterion in my case was that the algoritmh iterates as long as the change in the feature subset improves the model's performance. When I add a feature that doesn't contribute to the model's performance, I stop it. Forward greedy feature selection is computationally efficient and often yields reasonably good results, especially when dealing with high-dimensional datasets. However, it may not always guarantee the globally optimal feature subset, as the decisions made at each step are based on local improvements rather than a comprehensive evaluation of all possible subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67bfecdf-c8e5-4bc6-a26d-36ca492b9be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added feature is: mean concave points.\n",
      "With new feature set of size 1, we achieve accuracy of: 0.914551282051282.\n",
      "Added feature is: worst texture.\n",
      "With new feature set of size 2, we achieve accuracy of: 0.9347435897435897.\n",
      "Added feature is: worst area.\n",
      "With new feature set of size 3, we achieve accuracy of: 0.9648076923076923.\n",
      "Added feature is: texture error.\n",
      "With new feature set of size 4, we achieve accuracy of: 0.9748076923076923.\n",
      "Added feature is: compactness error.\n",
      "\n",
      "\n",
      "WARNING: Stoping the algorithm because of the worse performance!\n",
      "With new feature set of size 5, we achieve accuracy of: 0.9748076923076923.\n",
      "\n",
      "The optimal features are: ['mean concave points', 'worst texture', 'worst area', 'texture error'], with the feature size 4.\n",
      "The final accuracy we get is: 0.9748076923076923.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "# Here I define the feature list from which I add to my model to select the best ones\n",
    "remaining_features = list(range(X.shape[1]))\n",
    "\n",
    "# I define the selected features array with which I train model and evaluate it\n",
    "selected_features = []\n",
    "\n",
    "# We also need to define a model accuracy to compare it when adding new features\n",
    "model_performance = -np.inf\n",
    "\n",
    "while True:\n",
    "    # I define the temporary best score and best feature through the loop\n",
    "    best_score = -np.inf\n",
    "    best_feature = None\n",
    "\n",
    "    # Then I iterate through all features to see which one is the best addition\n",
    "    for f in remaining_features:\n",
    "        chosen_features = selected_features + [f]\n",
    "\n",
    "        # These are the features I will evaluate my model with\n",
    "        X_subset = X_train[:, chosen_features]\n",
    "\n",
    "        # Using SVC as the classifier\n",
    "        svc = SVC()\n",
    "\n",
    "        # Performing 10-fold cross-validation\n",
    "        cv_scores = cross_val_score(svc, X_subset, y_train, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42))\n",
    "        average_score = np.mean(cv_scores)\n",
    "\n",
    "        # We check if the feature we added is the best one yet\n",
    "        if(average_score > best_score):\n",
    "            best_score = average_score\n",
    "            best_feature = f\n",
    "\n",
    "    # We update our optimal feature list\n",
    "    selected_features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    print(f\"Added feature is: {data['feature_names'][best_feature]}.\")\n",
    "\n",
    "\n",
    "    # We check if the model performed better with the added feature or should we stop it\n",
    "    if(best_score > model_performance):\n",
    "        print(f\"With new feature set of size {len(selected_features)}, we achieve accuracy of: {best_score}.\")\n",
    "        model_performance = best_score\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(f\"WARNING: Stoping the algorithm because of the worse performance!\")\n",
    "        print(f\"With new feature set of size {len(selected_features)}, we achieve accuracy of: {best_score}.\")\n",
    "        selected_features.remove(best_feature)\n",
    "        print(\"\")\n",
    "        final_features = [data['feature_names'][i] for i in selected_features]\n",
    "        print(f\"The optimal features are: {final_features}, with the feature size {len(final_features)}.\")\n",
    "        print(f\"The final accuracy we get is: {model_performance}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92501ba9-a0f5-4800-a624-2c8c39648a5a",
   "metadata": {},
   "source": [
    "As seen, if I set the criteria for ending the algorithm to be that the model's performance HAS to improve and can't even stay the same (best_score > model_performance), I get that the optimal number of features is 4, with which I achieve accuracy of 0.9748076923076923. Those features are: \n",
    "1. mean concave points\n",
    "2. worst texture\n",
    "3. worst area\n",
    "4. texture error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec091b1b-fd5a-4da0-ab9c-9be6b1cc153c",
   "metadata": {},
   "source": [
    "### Subtask 2 - Backward Greedy Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a96fd-dc6d-4cd3-8802-9797f63fd9fb",
   "metadata": {},
   "source": [
    "In the second subtask, I implement 'Backward greedy feature selection' algorithm. It is another heuristic approach used for feature selection, offering a complementary strategy to forward selection. Unlike forward selection, backward feature selection starts with the entire set of features and iteratively removes the least important ones based on a chosen criterion, which in our case is a 10-fold cross validation. The process continues until a predefined stopping condition is met, which is in this case the performance of our model after removed feature being worse than in the last step. Backward feature selection can be advantageous in scenarios where the initial feature space is large, as it systematically trims down the set of features, potentially reducing overfitting and improving model interpretability. It also consideres dependencies between features. However, similar to forward selection, it may not always guarantee the globally optimal subset and could be sensitive to the choice of the stopping criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca5b3c57-b183-4e8b-a7b1-7fca4e49f6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed feature is: symmetry error.\n",
      "With new feature set of size 29, we achieve accuracy of: 0.9722435897435897.\n",
      "Removed feature is: mean radius.\n",
      "With new feature set of size 28, we achieve accuracy of: 0.9722435897435897.\n",
      "Removed feature is: mean perimeter.\n",
      "With new feature set of size 27, we achieve accuracy of: 0.9722435897435897.\n",
      "Removed feature is: mean smoothness.\n",
      "With new feature set of size 26, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: mean symmetry.\n",
      "With new feature set of size 25, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: mean area.\n",
      "With new feature set of size 24, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: radius error.\n",
      "With new feature set of size 23, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: worst texture.\n",
      "With new feature set of size 22, we achieve accuracy of: 0.9748076923076923.\n",
      "Removed feature is: mean compactness.\n",
      "With new feature set of size 21, we achieve accuracy of: 0.9748076923076923.\n",
      "Removed feature is: fractal dimension error.\n",
      "With new feature set of size 20, we achieve accuracy of: 0.9773076923076923.\n",
      "Removed feature is: concavity error.\n",
      "With new feature set of size 19, we achieve accuracy of: 0.9773076923076923.\n",
      "Removed feature is: worst area.\n",
      "With new feature set of size 18, we achieve accuracy of: 0.9773076923076923.\n",
      "Removed feature is: mean concavity.\n",
      "\n",
      "\n",
      "WARNING: Stoping the algorithm because of the worse performance!\n",
      "With new feature set of size 17, we achieve accuracy of: 0.9748076923076923.\n",
      "\n",
      "The optimal features are: ['mean texture', 'mean concave points', 'mean fractal dimension', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concave points error', 'worst radius', 'worst perimeter', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', 'mean concavity'], with the feature size 18.\n",
      "The final accuracy we get is: 0.9773076923076923.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "# Here I define the feature list from which I add to my model to select the best ones\n",
    "remaining_features = list(range(X.shape[1]))\n",
    "\n",
    "# We also need to define a model accuracy to compare it when adding new features\n",
    "model_performance = -np.inf\n",
    "\n",
    "while True:\n",
    "    # I define the temporary best score and best feature through the loop\n",
    "    best_score = -np.inf\n",
    "    worst_feature = None\n",
    "\n",
    "    # Then I iterate through all features to see which one is the best addition\n",
    "    for f in remaining_features:\n",
    "        chosen_features = remaining_features.copy()\n",
    "        chosen_features.remove(f)\n",
    "\n",
    "        # These are the features I will evaluate my model with\n",
    "        X_subset = X_train[:, chosen_features]\n",
    "\n",
    "        # Using SVC as the classifier\n",
    "        svc = SVC()\n",
    "\n",
    "        # Performing 10-fold cross-validation\n",
    "        cv_scores = cross_val_score(svc, X_subset, y_train, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42))\n",
    "        average_score = np.mean(cv_scores)\n",
    "\n",
    "        # We check if the feature we added is the best one yet\n",
    "        if(average_score > best_score):\n",
    "            best_score = average_score\n",
    "            worst_feature = f\n",
    "\n",
    "    # We update our optimal feature list\n",
    "    remaining_features.remove(worst_feature)\n",
    "    print(f\"Removed feature is: {data['feature_names'][worst_feature]}.\")\n",
    "\n",
    "    \n",
    "    # We check if the model performed better with the added feature or should we stop it\n",
    "    if(best_score >= model_performance):\n",
    "        print(f\"With new feature set of size {len(remaining_features)}, we achieve accuracy of: {best_score}.\")\n",
    "        model_performance = best_score\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(f\"WARNING: Stoping the algorithm because of the worse performance!\")\n",
    "        print(f\"With new feature set of size {len(remaining_features)}, we achieve accuracy of: {best_score}.\")\n",
    "        remaining_features.append(worst_feature)\n",
    "        print(\"\")\n",
    "        final_features = [data['feature_names'][i] for i in remaining_features]\n",
    "        print(f\"The optimal features are: {final_features}, with the feature size {len(final_features)}.\")\n",
    "        print(f\"The final accuracy we get is: {model_performance}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a020b678-2058-4207-8e65-2201dd7b667f",
   "metadata": {},
   "source": [
    "As you can see, I have removed 12 features to get the optimal accuracy of 0.9773076923076923. The features that I removed are:\n",
    "1. symmetry error\n",
    "2. mean radius\n",
    "3. mean perimeter\n",
    "4. mean smoothnes\n",
    "5. mean symmetry\n",
    "6. mean area\n",
    "7. radius error\n",
    "8. worst texture\n",
    "9. mean compactness\n",
    "10. fractal dimension error\n",
    "11. concavity error\n",
    "12. worst area\n",
    "\n",
    "We can also see that the model accuracy stays the same often when removing the worst features, but we keep on goint since our goal is to have as small features subset as possible while being sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a90a05-5026-4df5-81a3-fe470c926826",
   "metadata": {},
   "source": [
    "### Subtask 3 - Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf81adb-d4ec-47a5-a9e5-ac68189ddddb",
   "metadata": {},
   "source": [
    "#### Part 1 - Forward greedy feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019d106-1301-4255-81f8-65fec2a555a9",
   "metadata": {},
   "source": [
    "In this form of forward greedy feature selection I do mostly the same thing, except the algorithm stopping criteria, where I change it to stop after 6 iterations since six optimal features are choosen after it. I just add a iteration counter that I increase in each iteration and I check it's value in each iteration to see when to stop the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df579386-9ed5-43bf-a827-cd6e98ddfe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added feature is: mean concave points.\n",
      "With new feature set of size 1, we achieve accuracy of: 0.914551282051282.\n",
      "\n",
      "Added feature is: worst texture.\n",
      "With new feature set of size 2, we achieve accuracy of: 0.9347435897435897.\n",
      "\n",
      "Added feature is: worst area.\n",
      "With new feature set of size 3, we achieve accuracy of: 0.9648076923076923.\n",
      "\n",
      "Added feature is: texture error.\n",
      "With new feature set of size 4, we achieve accuracy of: 0.9748076923076923.\n",
      "\n",
      "Added feature is: compactness error.\n",
      "With new feature set of size 5, we achieve accuracy of: 0.9748076923076923.\n",
      "\n",
      "Added feature is: worst smoothness.\n",
      "With new feature set of size 6, we achieve accuracy of: 0.9773717948717948.\n",
      "\n",
      "\n",
      "WARNING: Stoping the algorithm because of the number of features!\n",
      "\n",
      "The optimal features are: ['mean concave points', 'worst texture', 'worst area', 'texture error', 'compactness error', 'worst smoothness'].\n",
      "The final accuracy we get is: 0.9773717948717948.\n"
     ]
    }
   ],
   "source": [
    "# Here I define the feature list from which I add to my model to select the best ones\n",
    "remaining_features = list(range(X.shape[1]))\n",
    "\n",
    "# I define the selected features array with which I train model and evaluate it\n",
    "selected_features = []\n",
    "\n",
    "# We also need to define a model accuracy to compare it when adding new features\n",
    "model_performance = -np.inf\n",
    "\n",
    "# Adding the iteration counter for stopping the algorithm\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    # I define the temporary best score and best feature through the loop\n",
    "    best_score = -np.inf\n",
    "    best_feature = None\n",
    "    \n",
    "    # Then I iterate through all features to see which one is the best addition\n",
    "    for f in remaining_features:\n",
    "        chosen_features = selected_features + [f]\n",
    "\n",
    "        # These are the features I will evaluate my model with\n",
    "        X_subset = X_train[:, chosen_features]\n",
    "\n",
    "        # Using SVC as the classifier\n",
    "        svc = SVC()\n",
    "\n",
    "        # Performing 10-fold cross-validation\n",
    "        cv_scores = cross_val_score(svc, X_subset, y_train, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42))\n",
    "        average_score = np.mean(cv_scores)\n",
    "\n",
    "        # We check if the feature we added is the best one yet\n",
    "        if(average_score > best_score):\n",
    "            best_score = average_score\n",
    "            best_feature = f\n",
    "\n",
    "    # We update our optimal feature list\n",
    "    selected_features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    iteration += 1\n",
    "\n",
    "    # We check if the model performed better with the added feature or should we stop it\n",
    "    if(iteration < 7):\n",
    "        print(f\"Added feature is: {data['feature_names'][best_feature]}.\")\n",
    "        print(f\"With new feature set of size {len(selected_features)}, we achieve accuracy of: {best_score}.\")\n",
    "        print(\"\")\n",
    "        model_performance = best_score\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(f\"WARNING: Stoping the algorithm because of the number of features!\")\n",
    "        selected_features.remove(best_feature)\n",
    "        print(\"\")\n",
    "        final_features = [data['feature_names'][i] for i in selected_features]\n",
    "        print(f\"The optimal features are: {final_features}.\")\n",
    "        print(f\"The final accuracy we get is: {model_performance}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c1c08-cb3e-4b03-8a6e-f2a1a7d73d5a",
   "metadata": {},
   "source": [
    "With forward greedy feature selection, the 6 features that I choose for my algorithm are:\n",
    "1. mean concave points\n",
    "2. worst texture\n",
    "3. worst area\n",
    "4. texture error\n",
    "5. compactness error\n",
    "6. worst smoothness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132e2b0-e7fa-4385-89f6-c0954a8bd9a0",
   "metadata": {},
   "source": [
    "#### Part 2 - Backward greedy feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa04548-5878-479b-8e2a-cabc5df48e30",
   "metadata": {},
   "source": [
    "With this type of backward greedy feature selection I also start off the same, and in each step I check the remaining number of features from the optimal feature subset. Since I remove some features in each iteration, it's size changes and when the size of the subset reaches 6, I stop the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "852602f7-1761-48a1-bd1c-cd61f355f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed feature is: symmetry error.\n",
      "With new feature set of size 29, we achieve accuracy of: 0.9722435897435897.\n",
      "Removed feature is: mean radius.\n",
      "With new feature set of size 28, we achieve accuracy of: 0.9722435897435897.\n",
      "Removed feature is: mean perimeter.\n",
      "With new feature set of size 27, we achieve accuracy of: 0.9722435897435897.\n",
      "Removed feature is: mean smoothness.\n",
      "With new feature set of size 26, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: mean symmetry.\n",
      "With new feature set of size 25, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: mean area.\n",
      "With new feature set of size 24, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: radius error.\n",
      "With new feature set of size 23, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: worst texture.\n",
      "With new feature set of size 22, we achieve accuracy of: 0.9748076923076923.\n",
      "Removed feature is: mean compactness.\n",
      "With new feature set of size 21, we achieve accuracy of: 0.9748076923076923.\n",
      "Removed feature is: fractal dimension error.\n",
      "With new feature set of size 20, we achieve accuracy of: 0.9773076923076923.\n",
      "Removed feature is: concavity error.\n",
      "With new feature set of size 19, we achieve accuracy of: 0.9773076923076923.\n",
      "Removed feature is: worst area.\n",
      "With new feature set of size 18, we achieve accuracy of: 0.9773076923076923.\n",
      "Removed feature is: mean concavity.\n",
      "With new feature set of size 17, we achieve accuracy of: 0.9748076923076923.\n",
      "Removed feature is: smoothness error.\n",
      "With new feature set of size 16, we achieve accuracy of: 0.9798076923076924.\n",
      "Removed feature is: concave points error.\n",
      "With new feature set of size 15, we achieve accuracy of: 0.9798076923076924.\n",
      "Removed feature is: worst fractal dimension.\n",
      "With new feature set of size 14, we achieve accuracy of: 0.9823076923076923.\n",
      "Removed feature is: texture error.\n",
      "With new feature set of size 13, we achieve accuracy of: 0.9823076923076923.\n",
      "Removed feature is: perimeter error.\n",
      "With new feature set of size 12, we achieve accuracy of: 0.9798076923076924.\n",
      "Removed feature is: compactness error.\n",
      "With new feature set of size 11, we achieve accuracy of: 0.9798076923076924.\n",
      "Removed feature is: worst compactness.\n",
      "With new feature set of size 10, we achieve accuracy of: 0.9798076923076924.\n",
      "Removed feature is: worst concavity.\n",
      "With new feature set of size 9, we achieve accuracy of: 0.9823076923076923.\n",
      "Removed feature is: area error.\n",
      "With new feature set of size 8, we achieve accuracy of: 0.9773076923076923.\n",
      "Removed feature is: worst perimeter.\n",
      "With new feature set of size 7, we achieve accuracy of: 0.9747435897435898.\n",
      "Removed feature is: mean fractal dimension.\n",
      "With new feature set of size 6, we achieve accuracy of: 0.9697435897435899.\n",
      "Removed feature is: worst concave points.\n",
      "\n",
      "WARNING: Stoping the algorithm because of the number of features!\n",
      "\n",
      "The optimal features are: ['mean texture', 'mean concave points', 'worst radius', 'worst smoothness', 'worst symmetry', 'worst concave points'].\n",
      "The final accuracy we get is: 0.9697435897435899.\n"
     ]
    }
   ],
   "source": [
    "# Here I define the feature list from which I add to my model to select the best ones\n",
    "remaining_features = list(range(X.shape[1]))\n",
    "\n",
    "# We also need to define a model accuracy to compare it when adding new features\n",
    "model_performance = -np.inf\n",
    "\n",
    "while True:\n",
    "    # I define the temporary best score and best feature through the loop\n",
    "    best_score = -np.inf\n",
    "    worst_feature = None\n",
    "\n",
    "    # Then I iterate through all features to see which one is the best addition\n",
    "    for f in remaining_features:\n",
    "        chosen_features = remaining_features.copy()\n",
    "        chosen_features.remove(f)\n",
    "\n",
    "        # These are the features I will evaluate my model with\n",
    "        X_subset = X_train[:, chosen_features]\n",
    "\n",
    "        # Using SVC as the classifier\n",
    "        svc = SVC()\n",
    "\n",
    "        # Performing 10-fold cross-validation\n",
    "        cv_scores = cross_val_score(svc, X_subset, y_train, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42))\n",
    "        average_score = np.mean(cv_scores)\n",
    "\n",
    "        # We check if the feature we added is the best one yet\n",
    "        if(average_score > best_score):\n",
    "            best_score = average_score\n",
    "            worst_feature = f\n",
    "\n",
    "    # We update our optimal feature list\n",
    "    remaining_features.remove(worst_feature)\n",
    "    print(f\"Removed feature is: {data['feature_names'][worst_feature]}.\")\n",
    "\n",
    "    \n",
    "    # We check if the model performed better with the added feature or should we stop it\n",
    "    if(len(remaining_features) >= 6):\n",
    "        print(f\"With new feature set of size {len(remaining_features)}, we achieve accuracy of: {best_score}.\")\n",
    "        model_performance = best_score\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(f\"WARNING: Stoping the algorithm because of the number of features!\")\n",
    "        remaining_features.append(worst_feature)\n",
    "        print(\"\")\n",
    "        final_features = [data['feature_names'][i] for i in remaining_features]\n",
    "        print(f\"The optimal features are: {final_features}.\")\n",
    "        print(f\"The final accuracy we get is: {model_performance}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378771b-d32e-4b94-861f-de5f7cfdce4d",
   "metadata": {},
   "source": [
    "With the backward greedy feature selection algorithm, the 6 features that I choose are:\n",
    "1. mean texture\n",
    "2. mean concave points\n",
    "3. worst radius\n",
    "4. worst smoothness\n",
    "5. worst symmetry\n",
    "6. worst concave points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a38129-74d4-41c0-8035-6ed90663c16f",
   "metadata": {},
   "source": [
    "The common optimal features I calculated with both algorithms are 'Mean concave points' and 'Worst smoothness', while the rest are differet. The reason they differ is beacause forward greedy feature selection is prone to only choose local optimal solutions without looking at the rest, where on the other hand, backward greedy feature selection also takes dependencies of features into count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2925d-e937-4732-815c-6125749a0104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
