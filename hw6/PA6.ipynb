{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5864f173-a24f-4e85-a85d-379cd726b0a6",
   "metadata": {},
   "source": [
    "# PA6\n",
    "## Predrag Dindic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc1229-289d-4b04-907d-d204c9d982d3",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "It’s Christmas time (I know, it’s post Christmas time already. . .) and Santa has many\n",
    "presents to deliver. He can’t deliver presents to all houses but has to selected a subset\n",
    "of all houses to optimize certain objectives (see below). Presents can only be efficiently\n",
    "delivered through chimneys and it is much more time consuming to deliver them in other\n",
    "(magical) ways. Unfortunately, in recent years, more and more houses without chimneys\n",
    "are built, making the planning of the delivery process more and more challenging. Santa’s\n",
    "objective is to, in some subtasks, in particular deliver presents to children. Luckily, we\n",
    "can leverage ideas from machine learning and decision theory to support the planning.\n",
    "The data for this task is available at [1] and has the following features:\n",
    "* x and y coordinates of a house (dimensions 0 and 1)\n",
    "* age a of a house (dimension 2)\n",
    "* average carbon-dioxide emission c at the house (dimension 3)\n",
    "* distance to next school (dimension 4)\n",
    "* average number of children living in houses in the neighborhood (dimension 5)\n",
    "The training data also comes with the following labels:\n",
    "* House has a chimney (dimension 0)\n",
    "(“true”/“false” encoded as “1”/“0”)\n",
    "* Are children living in the house (dimension 1)\n",
    "(“true”/“false” encoded as “1”/“0”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de508ce7-3839-4c88-abbe-361d2d1ec5b0",
   "metadata": {},
   "source": [
    "### Subtask 1: Kernelized Logistic Regression\n",
    "Implement regularized kernelized logistic regression. Your implementation should accept\n",
    "a kernel function which allows to compute the kernel function (for fast execution, this\n",
    "kernel function should ideally support the efficient computation of the kernel matrix\n",
    "for multiple data points in a single call). For optimization, feel free to use the Adam\n",
    "optimizer.\n",
    "* Briefly describe how you implement learning and prediction.\n",
    "\n",
    "Evaluate and test your kernelized logistic regression function on the regression data\n",
    "available from [1] using an RBF kernel for predicting whether a house has a chimney or\n",
    "not (i.e., solve a binary classification problem) computed only on the geographic distance\n",
    "between data points. You might want to standardize the scale of the data if that improves performance.\n",
    "* Report classification accuracy on the training and validation data.\n",
    "Runtime is less than 1 s for 100 iterations of gradient descent (including kernel computation, etc. but no hyperparameter tuning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2110022d-62c2-4eaf-ba1d-29eb8529a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Predrag\\AppData\\Local\\Temp\\ipykernel_8408\\627278128.py:31: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6590\n",
      "Testing Accuracy: 0.6390\n",
      "Optimal parameters are: gamma - 0.01, alpha - 5.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma):\n",
    "    \"\"\"\n",
    "    Function for implementing the rbf kernel for given coordinates and gamma parameter.\n",
    "    \"\"\"\n",
    "    return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)\n",
    "\n",
    "\n",
    "def compute_kernel_matrix(X, kernel_func, gamma):\n",
    "    \"\"\"\n",
    "    Function for creating a kernel matrix with the data given, specified kernel function and gamma parameter.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i, n_samples):\n",
    "            K[i, j] = kernel_func(X[i], X[j], gamma)\n",
    "            K[j, i] = K[i, j]\n",
    "    return K\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Function for calculating the sigmoid function in logistic regression.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def regularized_kernelized_logistic_regression(X, y, K, gamma, alpha=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8, max_iter=100):\n",
    "    # Getting the number of samples\n",
    "    n_samples, _ = X.shape\n",
    "\n",
    "    # Initializing the parameters\n",
    "    theta = np.zeros(n_samples)\n",
    "    m = np.zeros_like(theta)\n",
    "    v = np.zeros_like(theta)\n",
    "    t = 0\n",
    "\n",
    "    # Optimizing with Adam\n",
    "    for _ in range(max_iter):\n",
    "        t += 1\n",
    "\n",
    "        # Gradient\n",
    "        z = K @ theta  \n",
    "        predictions = sigmoid(z)\n",
    "        gradient = -(1/n_samples) * (K.T @ (y - predictions)) + (alpha / n_samples) * (K @ theta)\n",
    "\n",
    "        # Updating moment estimates\n",
    "        m = beta1 * m + (1 - beta1) * gradient\n",
    "        v = beta2 * v + (1 - beta2) * gradient**2\n",
    "\n",
    "        # Corrected moment estimates for the update\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "\n",
    "        # Updating the weights\n",
    "        theta -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return theta\n",
    "\n",
    "def predict(X_train, theta, kernel_func, gamma):\n",
    "    \"\"\"\n",
    "    Function for predicting with a sigmoid function by given data, weights, kernel function and gamma parameter.\n",
    "    \"\"\"\n",
    "    K_test = compute_kernel_matrix(X_train, kernel_func, gamma)\n",
    "    predictions = sigmoid(K_test @ theta)\n",
    "    return predictions\n",
    "\n",
    "# Loading the data\n",
    "hf = h5py.File('regression.h5', 'r')\n",
    "x_train = np.array(hf.get('x_train'))\n",
    "y_train_chimney = np.array(hf.get('y_train')[:, 0])  # House has a chimney (dimension 0)\n",
    "x_test = np.array(hf.get('x_test'))\n",
    "y_test_chimney = np.array(hf.get('y_test')[:, 0])  # House has a chimney (dimension 0)\n",
    "hf.close()\n",
    "\n",
    "# Using only features for the distance (coordinates and distance to next school)\n",
    "x_train = x_train[:, [0, 4]]\n",
    "x_test = x_test[:, [0, 4]]\n",
    "\n",
    "# Standardizing\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Defining the hyperparameters\n",
    "beta1 = 0.8\n",
    "beta2 = 0.99\n",
    "epsilon = 1e-8\n",
    "max_iter = 100\n",
    "\n",
    "# Defining values of alpha and gamma we will trying to find the optimal model\n",
    "gammas = [0.01, 0.1]\n",
    "alphas = [10, 5, 1, 0.1, 1e-3, 1e-5]\n",
    "\n",
    "# Initializing the variables for finding the optimal hyperparameters\n",
    "best_model = None\n",
    "best_score_train = 0\n",
    "best_score_test = 0\n",
    "best_gamma = 0\n",
    "best_alpha = 0\n",
    "\n",
    "for g in gammas:\n",
    "    for a in alphas:\n",
    "        # Creating the kernel matrix\n",
    "        K = compute_kernel_matrix(x_train_scaled, rbf_kernel, g)\n",
    "\n",
    "        # Training\n",
    "        theta = regularized_kernelized_logistic_regression(x_train_scaled, y_train_chimney, K, g, a, beta1, beta2, epsilon, max_iter)\n",
    "        \n",
    "        # Predicting\n",
    "        predictions_train = predict(x_train_scaled, theta, rbf_kernel, g)\n",
    "        predictions_test = predict(x_test_scaled, theta, rbf_kernel, g)\n",
    "        \n",
    "        # Converting to binary\n",
    "        predictions_train_binary = (predictions_train >= 0.5).astype(int)\n",
    "        predictions_test_binary = (predictions_test >= 0.5).astype(int)\n",
    "        \n",
    "        # Calculating accuracy\n",
    "        accuracy_train = np.mean(predictions_train_binary == y_train_chimney)\n",
    "        accuracy_test = np.mean(predictions_test_binary == y_test_chimney)\n",
    "\n",
    "        # Checking if we got the best accuracy (optimal hyperparameters) so far\n",
    "        if(accuracy_test > best_score_test):\n",
    "            best_score_test = accuracy_test\n",
    "            best_score_train = accuracy_train\n",
    "            best_gamma = g\n",
    "            best_alpha = a\n",
    "\n",
    "print(f\"Training Accuracy: {best_score_train:.4f}\")\n",
    "print(f\"Testing Accuracy: {best_score_test:.4f}\")\n",
    "print(f\"Optimal parameters are: gamma - {best_gamma}, alpha - {best_alpha}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8563c-d981-45d1-b72c-a758028faca3",
   "metadata": {},
   "source": [
    "I am starting off this subtask by creating a RBF function which implements the RBF kernel for given parameters, which we use for creating the kernel matrix. For the creation of kernel matrix I use the next function, which just applies the kernel function to the data and stores it into the matrix. After that I define a sigmoid function used in logistic regression for fitting the data. After that I define the model function, that starts by initializing an weight vector with all the zeros, which I update with Adam in a for loop. In that loop, we calculate the first and second moments of a gradient for each weight in number of iterations we are given, and after that, we use them to compute our optimal weights. At the end, there is a function for predicting where we create out kernel matrix, and use the sigmoid function we defined with the optimal weights we got to compute the probabilities of each point belonging to each target feature (chimney / no chimney). When those predictions are after made, I use a threshold of 0.5 to tag each prediction to 0 or 1 so I can binary classify them. I also iterate over different values of gamma and learning rate (alpha) to find the optimal hyperparameters for the best accuracy. The best testing accuracy I end up with is around 0.64%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe61084-b7b2-4944-ba9c-f047a0fcf7d5",
   "metadata": {},
   "source": [
    "### Subtask 2: Improving your kernel\n",
    "Improve the performance of kernelized logistic regression for the classification problem\n",
    "from the previous subtask by using more features from the data in a customized kernel.\n",
    "* Describe the design of your kernel.\n",
    "\n",
    "If you need to choose hyper-parameters, use some variant for hyper-parameter search\n",
    "and cross-validation to do so.\n",
    "* Report the classification accuracy of your improved kernel on the training and validation data and the selected hyper-parameters (if any).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "050897fc-e70c-4b2d-bce8-870e6b0ee605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_rbf(x1, x2, gamma):\n",
    "    \"\"\"\n",
    "    Kernel function which is just a combination of 3 rbf kernels, where the first one is set to power of 3 and the second one is set to power\n",
    "    of 2.\n",
    "    \"\"\"\n",
    "    return  rbf_kernel(x1, x2, gamma) ** 3 + rbf_kernel(x1, x2, gamma) **2 + rbf_kernel(x1, x2, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0851e79e-f0d4-435e-9ffc-0af4800c3f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Predrag\\AppData\\Local\\Temp\\ipykernel_8408\\627278128.py:31: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7560\n",
      "Testing Accuracy: 0.6400\n",
      "Optimal parameters are: gamma - 0.1, alpha - 0.001.\n"
     ]
    }
   ],
   "source": [
    "# Now I will be using all the features, not only the ones for coordinates\n",
    "hf = h5py.File('regression.h5', 'r')\n",
    "x_train = np.array(hf.get('x_train'))\n",
    "y_train_chimney = np.array(hf.get('y_train')[:, 0])  # House has a chimney (dimension 0)\n",
    "x_test = np.array(hf.get('x_test'))\n",
    "y_test_chimney = np.array(hf.get('y_test')[:, 0])  # House has a chimney (dimension 0)\n",
    "hf.close()\n",
    "\n",
    "# Standardizing\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "for g in gammas:\n",
    "    for a in alphas:\n",
    "        # Creating the kernel matrix\n",
    "        K = compute_kernel_matrix(x_train_scaled, combined_rbf, g)\n",
    "\n",
    "        # Training\n",
    "        theta = regularized_kernelized_logistic_regression(x_train_scaled, y_train_chimney, K, g, a, beta1, beta2, epsilon, max_iter)\n",
    "        \n",
    "        # Predicting\n",
    "        predictions_train = predict(x_train_scaled, theta, rbf_kernel, g)\n",
    "        predictions_test = predict(x_test_scaled, theta, rbf_kernel, g)\n",
    "        \n",
    "        # Converting to binary\n",
    "        predictions_train_binary = (predictions_train >= 0.5).astype(int)\n",
    "        predictions_test_binary = (predictions_test >= 0.5).astype(int)\n",
    "        \n",
    "        # Calculating accuracy\n",
    "        accuracy_train = np.mean(predictions_train_binary == y_train_chimney)\n",
    "        accuracy_test = np.mean(predictions_test_binary == y_test_chimney)\n",
    "\n",
    "        # Checking if we got the best accuracy (optimal hyperparameters) so far\n",
    "        if(accuracy_test > best_score_test):\n",
    "            best_score_test = accuracy_test\n",
    "            best_score_train = accuracy_train\n",
    "            best_gamma = g\n",
    "            best_alpha = a\n",
    "\n",
    "print(f\"Training Accuracy: {best_score_train:.4f}\")\n",
    "print(f\"Testing Accuracy: {best_score_test:.4f}\")\n",
    "print(f\"Optimal parameters are: gamma - {best_gamma}, alpha - {best_alpha}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7cad97-996c-4885-bcd8-38431d53abd7",
   "metadata": {},
   "source": [
    "As you can see with the help of my custom kernel I improved accuracy just a bit, by having a more precise and complicated kernel would for sure even more improve it, but I choose a simple one. The kernel I applied is a combination of multiple RBF kernel functions, where I multiply them and add them up. The first one is set to the power of three, the second to the power of two and the last one to the power of one. The optimal parameters I got are gamma being 0.1 and alpha being 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de788ed2-f8a6-4be0-a406-1de7eae0aa24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
